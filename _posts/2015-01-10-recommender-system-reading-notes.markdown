---
layout: post
title: "推荐引擎相关文章阅读笔记"
categories: Notes
---
Source
----------------
[探索推荐引擎内部的秘密][1]  

<hr />

第 1 部分: 推荐引擎初探
------------------------------

###信息发现
推荐引擎针对用户不明确自己的需要却需要符合自己口味和喜好的结果。在用户明确自己需求的时候，搜索引擎可以快速的帮用户查找需要的信息。

+ 推荐引擎  
数据源：要推荐物品或内容的元数据，系统用户的基本信息，用户对物品或者信息的偏好
    + 偏好信息可分两类：显式的用户反馈，隐式的用户反馈

+ 推荐引擎的分类    
根据分类标准有不同的分类：

    1. 根据是否为不同的用户推荐不同的数据：推荐引擎可以分为基于大众行为的推荐引擎和个性化推荐引擎

    2. 根据推荐引擎的数据源，即如何发现数据的相关性：
        + 根据系统用户的基本信息发现用户的相关程度，这种被称为基于人口统计学的推荐（Demographic-based Recommendation）
        + 根据推荐物品或内容的元数据，发现物品或者内容的相关性，这种被称为基于内容的推荐（Content-based Recommendation）
        + 根据用户对物品或者信息的偏好，发现物品或者内容本身的相关性，或者是发现用户的相关性，这种被称为基于协同过滤的推荐（Collaborative Filtering-based Recommendation）

    3. 根据推荐模型的建立方式：
        + 基于物品和用户本身的，预测每个用户对于每个物品的喜好程度
        + 基于关联规则的推荐（Rule-based Recommendation）：挖掘数据依赖关系，诸如“购物篮问题”
        + 基于模型的推荐（Model-based Recommendation）：用机器学习的方法用用户喜好信息训练出一个模型，然后基于该模型推荐

###深入推荐机制
1. 基于人口统计学的推荐：根据用户基本信息发现用户相关程度，然后将相似用户喜爱的物品/内容推荐。
    + 优点：
        1. 不使用当前用户对物品或内容的喜好历史，新用户没有cold start问题；
        2. 不依赖物品本身数据，domain-independent
    + 缺点：分类粗糙，有时推荐结果不精准；用户信息不好收集。
2. 基于内容的推荐：先根据要推荐的物品或内容元数据发掘相关性，然后根据用户喜好记录推荐相似的事物。
    + 优点：建模用户的兴趣，提供更精准的推荐
    + 缺点：
        1. 推荐的质量依赖于对物品建模的完整度
        2. 事物相似度的分析仅依赖事物本身的特征，没考虑人对事物的态度
        3. 需要基于用户的喜好历史推荐，对新用户有cold start问题
        这个模型应用广泛而且有不少成功案例， 诸如pandora
3. 基于协同过滤的推荐： 
可以分为三个子类：基于用户的推荐（User-based Recommendation），基于事物的推荐（Item-based Recommendation）和基于模型的推荐（Model-based Recommendation）

    1. 基于用户的系统过滤推荐   ：根据所有用户对物品或者信息的偏好，发现与当前用户口味和偏好相似的“邻居”用户群，在一般的应用中是采用计算“K- 邻居”的算法；然后，基于这 K 个邻居的历史偏好信息，为当前用户进行推荐。
    >“基于用户的协同过滤推荐机制和基于人口统计学的推荐机制都是计算用户的相似度，并基于“邻居”用户群计算推荐，但它们所不同的是如何计算用户的相似度，基于人口统计学的机制只考虑用户本身的特征，而基于用户的协同过滤机制可是在用户的历史偏好的数据上计算用户的相似度，它的基本假设是，喜欢类似物品的用户可能有相同或者相似的口味和偏好。”
    
    2. 基于事物的协同过滤推荐：基于项目的协同过滤推荐的基本原理也是类似的，只是说它使用所有用户对物品或者信息的偏好，发现物品和物品之间的相似度，然后根据用户的历史偏好信息，将类似的物品推荐给用户。“基于项目的协同过滤推荐和基于内容的推荐其实都是基于物品相似度预测推荐，只是相似度计算的方法不一样，前者是从用户历史的偏好推断，而后者是基于物品本身的属性特征信息”

    3. 基于模型的协同过滤推荐：基于模型的协同过滤推荐就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测，计算推荐。
        + 优点：
            1. 无需对物品或用户严格建模，domain-independent; 
            2. 计算出的推荐是开放的，支持发现用户的潜在兴趣 

        + 缺点：
            1. 基于历史数据，有cold start问题；
            2. 效果对历史数据的数量和准确度有依赖；3. 用户历史偏好的存储是用稀疏矩阵存储，有时错误的偏好会对推荐结果准确度有较大影响；4. 对有特殊品味用户推荐效果不佳；5. 对变化的适应度不够
    
4. 混合的推荐机制：加权的混合（Weighted Hybridization），切换的混合（Switching Hybridization），分区的混合（Mixed Hybridization），分层的混合（Meta-Level Hybridization）

<hr />

第 2 部分: 深入推荐引擎相关算法 - 协同过滤
------------------------------

###集体智慧和协同过滤   

####什么是集体智慧(Collective Intelligence)      

> 集体智慧是指在大量的人群的行为和数据中收集答案，帮助你对整个人群得到统计意义上的结论，这些结论是我们在单个个体上无法得到的，它往往是某种趋势或者人群中共性的部分。

####什么是协同过滤(Collaborative Filtering)
一个简单的例子：    

> 首先想一个简单的问题，如果你现在想看个电影，但你不知道具体看哪部，你会怎么做？大部分的人会问问周围的朋友，看看最近有什么好看的电影推荐，而我们一般更倾向于从口味比较类似的朋友那里得到推荐。这就是协同过滤的核心思想。

>协同过滤一般是在海量的用户中发掘出一小部分和你品位比较类似的，在协同过滤中，这些用户成为邻居，然后根据他们喜欢的其他东西组织成一个排序的目录作为推荐给你。

在推荐之前需要解决的问题：

+ 如何确定一个用户是不是和你有相似的品味？
+ 如何将邻居们的喜好组织成一个排序的目录？

####深入协同过滤的核心
基本步骤：收集用户偏好；找到相似事物；计算推荐  

**收集用户偏好**    
搜集用户行为的方式：评分，投票，转发，保存书签，Tag，点击流，页面停留时间，购买，etc.   

组合用户行为的方式: 

+   将不同的行为分组，如购买了这件商品的人还购买了……                            
+   根据不同行为反映用户喜好的程度将它们进行加权，得到用户对于物品的总体喜好。
    
预处理数据：减噪和归一化。
> 归一化：如前面讲到的，在计算用户对物品的喜好程度时，可能需要对不同的行为数据进行加权。但可以想象，不同行为的数据取值可能相差很大，比如，用户的查看数据必然比购买数据大的多，如何将各个行为的数据统一在一个相同的取值范围中，从而使得加权求和得到的总体喜好更加精确，就需要我们进行归一化处理。最简单的归一化处理，就是将各类数据除以此类中的最大值，以保证归一化后的数据取值在 [0,1] 范围中。 

####找到相似的用户或物品    
计算相似度的方法    

+ 欧几里得距离（Euclidean Distance）    
    ![欧几里得距离][2]      
    相似度转换  
    ![相似度][3]    

+ 皮尔逊相关系数（Pearson Correlation Coefficient） 
    皮尔逊相关系数一般用于计算两个定距变量间联系的紧密程度，它的取值在 [-1，+1] 之间。
    ![皮尔逊相关系数][4]    
    sx, sy是 x 和 y 的样品标准偏差。    

+ Cosine相似度（Cosine Similarity） 
    Cosine相似度应用于计算文档相似度：  
    ![Cosine相似度][5]  

+ Tanimoto系数（Tanimoto Coefficient）
    Tanimoto系数也称为Jaccard系数，是Cosine 相似度的扩展，也多用于计算文档数据的相似度：    
    ![Tanimoto系数][6]

计算相似邻居的方法      

+ 固定数量的邻居：K-neighborhoods 或者 Fix-size neighborhoods。即无论远近，只取最近的K个邻居。但是对于孤立点意义不大，因为孤立点周围邻居太少，要取不太相似的点做邻居。  
+ 基于相似度门槛的邻居：Threshold-based neighborhoods。取在距某点固定距离内的点，邻居点的个数不确定，但是相似度误差较小。  

####计算推荐

[1]: http://www.ibm.com/developerworks/cn/views/web/libraryview.jsp?view_by=search&sort_by=Date&sort_order=desc&view_by=Search&search_by=%E6%8E%A2%E7%B4%A2%E6%8E%A8%E8%8D%90%E5%BC%95%E6%93%8E%E5%86%85%E9%83%A8%E7%9A%84%E7%A7%98%E5%AF%86&dwsearch.x=12&dwsearch.y=11&dwsearch=Go  "探索推荐引擎内部的秘密"
[2]: http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/image003.gif "欧几里得距离"
[3]: http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/image005.gif "相似度公式"
[4]: http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/image007.gif "皮尔逊相关系数计算公式"
[5]: http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/image009.gif "Cosine相似度"
[6]: http://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy2/image011.gif "Tanimoto系数"


